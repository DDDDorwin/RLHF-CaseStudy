# RLHF-CaseStudy
This is a collection of materials related to Reinforcement Learning with Human Feedback (RLHF). And the repository will be continuously updated until I finish my thesis.

Welcome to  discuss and exchange ideas with me:)

<img width="963" alt="download" src="https://github.com/DDDDorwin/RLHF-CaseStudy/assets/89888473/4a2ff1da-ec86-4d30-9ffd-8a9bdbf79063">

## list
..* [Aligning ML Models with Human Feedback](https://github.com/HumanSignal/RLHF/tree/master)
... Label Studio




